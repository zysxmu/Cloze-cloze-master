{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# lstm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_path = './train-sentences.txt'\n",
    "\n",
    "word2vec_path = './GoogleNews-vectors-negative300.bin'\n",
    "\n",
    "\n",
    "\n",
    "num_layers = 2 # 2层RNN\n",
    "num_units = 300 # 隐藏单元数量\n",
    "use_peepholes = True # 是否使用peepholes\n",
    "\n",
    "input_keep_prob = 0.8 # 输入dropout\n",
    "output_keep_prob = 1.0 # 输出dropout\n",
    "\n",
    "batch_size = 32 # 每批数据的规模，每批有32个\n",
    "\n",
    "\n",
    "max_epoch = 500 # epoch\n",
    "\n",
    "isTraining = True # is training?\n",
    "\n",
    "# 读取unique word\n",
    "\n",
    "def _read_words(filename):\n",
    "    with tf.gfile.GFile(filename, \"r\") as f:\n",
    "        return f.read().replace(\"\\n\", \" \").split()\n",
    "\n",
    "# build word_to_id\n",
    "def _build_vocab(filename):\n",
    "    data = _read_words(filename)\n",
    "\n",
    "    counter = collections.Counter(data)\n",
    "    \n",
    "    count_pairs = sorted(counter.items(), key=lambda x: (-x[1], x[0]))\n",
    "\n",
    "    words, _ = list(zip(*count_pairs))\n",
    "    word_to_id = dict(zip(words, range(len(words))))\n",
    "\n",
    "    return word_to_id\n",
    "\n",
    "# load Embedding matrix (only contain our unique word)\n",
    "def loadEmbedding(word_to_id,word2vec_name):\n",
    "    \"\"\" Initialize embeddings with pre-trained word2vec vectors\n",
    "        Will modify the embedding weights of the current loaded model\n",
    "        Uses the GoogleNews pre-trained values\n",
    "    \"\"\"\n",
    "    \n",
    "    #initW = np.zeros((len(word_to_id), 5000)) \n",
    "    #return initW\n",
    "    \n",
    "    # Load the pre-trained word2vec data\n",
    "    with open(word2vec_name, \"rb\", 0) as f:\n",
    "        header = f.readline()\n",
    "        vocab_size, vector_size = map(int, header.split())\n",
    "        binary_len = np.dtype('float32').itemsize * vector_size\n",
    "        \n",
    "        # 如果word在GoogleNews pre-trained没有出现，就是设置为0\n",
    "            \n",
    "        initW = np.zeros((len(word_to_id), vector_size)) \n",
    "        for line in range(vocab_size):\n",
    "            word = []\n",
    "            while True:\n",
    "                ch = f.read(1)\n",
    "                if ch == b' ':\n",
    "                    word = b''.join(word).decode('utf-8')\n",
    "                    break\n",
    "                if ch != b'\\n':\n",
    "                    word.append(ch)\n",
    "            if word in word_to_id:\n",
    "                initW[word_to_id[word]] = np.fromstring(f.read(binary_len), dtype='float32')\n",
    "            else:\n",
    "                f.read(binary_len)\n",
    "    return initW\n",
    "\n",
    "\n",
    "# Load Google's pre-trained Word2Vec model.\n",
    "def get_word_to_id_word2vec(train_path):\n",
    "    \n",
    "    word_to_id = _build_vocab(train_path)\n",
    "    word2vec = loadEmbedding(word_to_id,word2vec_path)\n",
    "    \n",
    "    return word_to_id,word2vec,word2vec.shape[0],word2vec.shape[1]\n",
    "\n",
    "# 数据类型\n",
    "def data_type():\n",
    "    return tf.float32\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_to_id,word2vec,vocab_size,embedding_dim = get_word_to_id_word2vec(train_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def add_brnn(inputs,num_units,seq_lens,input_keep_prob,output_keep_prob,use_peepholes):\n",
    "        \n",
    "    ''' \n",
    "    inputs:\n",
    "        [\n",
    "        <tf.Tensor 'L_context:0' shape=(batch_size, time_step, num_units) dtype=float32>, \n",
    "        <tf.Tensor 'R_context:0' shape=(batch_size, time_step, num_units) dtype=float32>\n",
    "        ]\n",
    "        \n",
    "    seq_lens:\n",
    "        [\n",
    "        <tf.Tensor 'L_context_len:0' shape=(batch_size,) dtype=int32>, \n",
    "        <tf.Tensor 'R_context_len:0' shape=(batch_size,) dtype=int32>\n",
    "        ]\n",
    "    \n",
    "    return:\n",
    "    \n",
    "    [tf.concat(o[0],2,name='outputs') for o in batch_outputs]: # 左文和右文已经拼接\n",
    "        [\n",
    "        # output_fw\n",
    "        <tf.Tensor 'LSTM_1/outputs:0' shape=(batch_size, time_step, 2*num_units) dtype=float32>, \n",
    "        # output_bw\n",
    "        <tf.Tensor 'LSTM_1/outputs_1:0' shape=(batch_size, time_step, 2*num_units) dtype=float32>\n",
    "        ]\n",
    "    \n",
    "    [o[1] for o in batch_outputs]:\n",
    "    \n",
    "    # 左文\n",
    "        [\n",
    "        (LSTMStateTuple\n",
    "        # output_state_fw\n",
    "            (\n",
    "            c=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn/fw/fw/while/Exit_2:0' shape=(batch_size, num_units) dtype=float32>, \n",
    "            h=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn/fw/fw/while/Exit_3:0' shape=(batch_size, num_units) dtype=float32>\n",
    "            ), \n",
    "        # output_state_bw\n",
    "        LSTMStateTuple\n",
    "            (\n",
    "            c=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn/bw/bw/while/Exit_2:0' shape=(batch_size, num_units) dtype=float32>, \n",
    "            h=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn/bw/bw/while/Exit_3:0' shape=(batch_size, num_units) dtype=float32>\n",
    "            )\n",
    "        ), \n",
    "        \n",
    "    # 右文\n",
    "        (LSTMStateTuple\n",
    "            (\n",
    "        # output_state_fw\n",
    "            c=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn_1/fw/fw/while/Exit_2:0' shape=(batch_size, num_units) dtype=float32>, \n",
    "            h=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn_1/fw/fw/while/Exit_3:0' shape=(batch_size, num_units) dtype=float32>\n",
    "            ), \n",
    "        # output_state_bw\n",
    "        LSTMStateTuple\n",
    "            (\n",
    "            c=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn_1/bw/bw/while/Exit_2:0' shape=(batch_size, num_units) dtype=float32>, \n",
    "            h=<tf.Tensor 'LSTM_2/bi_dy_rnn/bidirectional_rnn_1/bw/bw/while/Exit_3:0' shape=(batch_size, num_units) dtype=float32>\n",
    "            )\n",
    "        )]\n",
    "        \n",
    "    '''\n",
    "    \n",
    "    # 左向，右向\n",
    "    cell_fw = tf.nn.rnn_cell.LSTMCell(num_units = num_units,use_peepholes = use_peepholes)\n",
    "    cell_fw = tf.nn.rnn_cell.DropoutWrapper(cell_fw,input_keep_prob=input_keep_prob,\n",
    "                                      output_keep_prob=output_keep_prob) # RNN只对多层之间的cell进行dropout \n",
    "    \n",
    "    cell_bw = tf.nn.rnn_cell.LSTMCell(num_units = num_units,use_peepholes = use_peepholes)\n",
    "    cell_bw = tf.nn.rnn_cell.DropoutWrapper(cell_bw,input_keep_prob=input_keep_prob,\n",
    "                                      output_keep_prob=output_keep_prob) # RNN只对多层之间的cell进行dropout \n",
    "    \n",
    "    # 记录输出\n",
    "    batch_outputs = []\n",
    "    \n",
    "    with tf.variable_scope(name_or_scope='bi_dy_rnn') as scope:\n",
    "        for input_sentences,seq_len in zip(inputs,seq_lens):\n",
    "           \n",
    "            batch_outputs.append(tf.nn.bidirectional_dynamic_rnn\n",
    "                           (cell_fw, cell_bw, input_sentences, sequence_length=seq_len, dtype=tf.float32))\n",
    "\n",
    "            scope.reuse_variables() # 第二次循环是右上下文，设置重用变量\n",
    "    \n",
    "    return [tf.concat(o[0],2,name='outputs') for o in batch_outputs], [o[1] for o in batch_outputs]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# first reset graph or will raise error in second time run\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# set word2vec into tensor\n",
    "embedding = tf.Variable(tf.constant(0.0, shape=[vocab_size, embedding_dim]),trainable=False, name=\"embedding\")\n",
    "embedding_placeholder = tf.placeholder(tf.float32, [vocab_size, embedding_dim])\n",
    "embedding_init = embedding.assign(embedding_placeholder)\n",
    "\n",
    "# input data\n",
    "L_context_id = tf.placeholder(tf.int32, shape=[None,None]) # left context word's id\n",
    "L_context_length = tf.placeholder(tf.int32, shape=[None])# left context word's len\n",
    "R_context_id = tf.placeholder(tf.int32, shape=[None,None])# left context word's id\n",
    "R_context_length = tf.placeholder(tf.int32, shape=[None])# left context word's len\n",
    "\n",
    "# get word2vec\n",
    "L_context_vec_before_drop = tf.nn.embedding_lookup(embedding, L_context_id,name='L_context_before_drop')\n",
    "R_context_vec_before_drop = tf.nn.embedding_lookup(embedding, R_context_id,name='R_context_before_drop')\n",
    "\n",
    "# input data dropout\n",
    "if isTraining and input_keep_prob < 1:\n",
    "    L_context_vec = tf.nn.dropout(L_context_vec_before_drop, input_keep_prob,name='L_context_vec')\n",
    "    R_context_vec = tf.nn.dropout(R_context_vec_before_drop, input_keep_prob,name='R_context_vec')\n",
    "\n",
    "with tf.variable_scope('LSTM_1'):\n",
    "    [L_outputs,R_outputs], _ = add_brnn(\n",
    "        [L_context_vec,R_context_vec], \n",
    "        num_units, \n",
    "        [L_context_length,R_context_length],\n",
    "        input_keep_prob,\n",
    "        output_keep_prob,\n",
    "        use_peepholes)\n",
    "\n",
    "with tf.variable_scope('LSTM_2'):\n",
    "    [L_outputs_2,R_outputs_2],[L_final_state,R_final_state] = add_brnn(\n",
    "        [L_outputs,R_outputs], \n",
    "        num_units, \n",
    "        [L_context_length,R_context_length],\n",
    "        input_keep_prob,\n",
    "        output_keep_prob,\n",
    "        use_peepholes)\n",
    "\n",
    "outputs = tf.concat([L_outputs,R_outputs], 1)#同一batch拼接在一起就是得到[文章，上下文特征向量]\n",
    "\n",
    "final_state = (\n",
    "    tf.concat([L_final_state[0][1], L_final_state[1][1]], 1) \n",
    "    + tf.concat([R_final_state[0][1], R_final_state[1][1]], 1))/2 #双向拼接、上下文取平均，得到encode向量\n",
    "\n",
    "\n",
    "#对填充位置进行mask，注意这里是softmax之前的mask，所以mask不是乘以0，而是减去1e12\n",
    "\n",
    "L_context_mask = (1-tf.cast(tf.sequence_mask(L_context_length), tf.float32))*(-1e12) \n",
    "\n",
    "R_context_mask = (1-tf.cast(tf.sequence_mask(R_context_length), tf.float32))*(-1e12)\n",
    "\n",
    "context_mask = tf.concat([L_context_mask,R_context_mask], 1)\n",
    "\n",
    "outputs = tf.concat([L_outputs,R_outputs], 1)#同一batch拼接在一起就是得到[文章，上下文特征向量]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with tf.Session() as sess:\n",
    "    \n",
    "    init = tf.global_variables_initializer()\n",
    "    sess.run(init)\n",
    "    \n",
    "    # 初始化embedding\n",
    "    sess.run(embedding_init,feed_dict={embedding_placeholder:word2vec})\n",
    "    \n",
    "    L_id = np.array([[71,72,73,70],[71,72,73,74]])\n",
    "    R_id = np.array([[75,76,77,78,70],[75,76,77,78,79]])\n",
    "    L_len = np.array([1,4])\n",
    "    R_len = np.array([1,3])\n",
    "    \n",
    "    ts,matrix = sess.run([L_context_mask,embedding], \n",
    "                         feed_dict={\n",
    "                             L_context_id:L_id,\n",
    "                             R_context_id:R_id,\n",
    "                             L_context_length:L_len, \n",
    "                             R_context_length:R_len})\n",
    "ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

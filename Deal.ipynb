{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 验证集答案评价函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def eval_pre(answer_list):\n",
    "    development_set_answers = open('./development_set_answers.txt').read()\n",
    "\n",
    "    development_set_answers = development_set_answers.split('\\n')\n",
    "\n",
    "    development_set_answers = development_set_answers[:-1]\n",
    "\n",
    "    for i in range(len(development_set_answers)):\n",
    "        development_set_answers[i] = \\\n",
    "        development_set_answers[i][development_set_answers[i].find('[')+1:development_set_answers[i].find(']')]\n",
    "\n",
    "    corr=0\n",
    "    if len(answer_dic)!=len(development_set_answers):\n",
    "        print('wrong')\n",
    "    for i in range(len(answer_list)):\n",
    "        if answer_list[i] == development_set_answers[i]:\n",
    "            corr+=1\n",
    "    print(corr,corr/len(answer_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# n-gram(2, 3, 4, 5) & 频率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## prepare"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取train set and dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "raw = open('./train-sentences.txt').read()\n",
    "train_words = raw.split('\\n')\n",
    "train_words = train_words[:-1]\n",
    "\n",
    "raw = open('./dev-sentences.txt').read()\n",
    "answer_sentences = raw.split('\\n')\n",
    "answer_sentences = answer_sentences[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算vocabular size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "\n",
    "# 计算vocabular size\n",
    "raw = open('./train-sentences.txt').read()\n",
    "raw = raw.replace('\\n',' ')\n",
    "raw = raw.split(' ')\n",
    "counter = collections.Counter(raw)\n",
    "\n",
    "voca_size = len(counter)\n",
    "print(voca_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## n-gram-model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 计算出现的频次"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def n_gram(n,train_words):\n",
    "    numerator = {}\n",
    "    denominator = {}\n",
    "    \n",
    "    numerator_list = []\n",
    "    denominator_list = []\n",
    "    \n",
    "    for i in range(len(train_words)):\n",
    "        \n",
    "        sentence = train_words[i].split(' ')\n",
    "        denominator_list = []\n",
    "        \n",
    "        for k in range(n - 1):\n",
    "            denominator_list.append('#')\n",
    "        \n",
    "        for k in range(0,len(sentence)):\n",
    "            \n",
    "            word_now = sentence[k]\n",
    "            \n",
    "            # 计算分母\n",
    "            denominator_string = ''\n",
    "            for word in denominator_list:\n",
    "                denominator_string +=word\n",
    "            \n",
    "            denominator[denominator_string] = denominator.get(denominator_string,0) + 1\n",
    "            \n",
    "            # 计算分子\n",
    "            numerator_string = denominator_string +  word_now\n",
    "            numerator[numerator_string] = numerator.get(numerator_string,0) + 1\n",
    "            \n",
    "            # 更新\n",
    "            for j in range(len(denominator_list) - 1):\n",
    "                denominator_list[j] = denominator_list[j+1]\n",
    "            denominator_list[-1] = word_now\n",
    "        \n",
    "        # 计算分母\n",
    "        denominator_string = ''\n",
    "        for word in denominator_list:\n",
    "            denominator_string +=word\n",
    "        denominator[denominator_string] = denominator.get(denominator_string,0) + 1\n",
    "        \n",
    "        # 计算分子\n",
    "        word_now = '&'\n",
    "        numerator_string = denominator_string +  word_now\n",
    "        numerator[numerator_string] = numerator.get(numerator_string,0) + 1\n",
    "\n",
    "        \n",
    "    return numerator,denominator\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到每个句子出现的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pre(n,numerator,denominator,answer_sentences,voca_size,smooth):\n",
    "    answer_dic = {}\n",
    "\n",
    "    for i in range(len(answer_sentences)):\n",
    "        \n",
    "        if i%6 == 0:\n",
    "            question_num = answer_sentences[i]\n",
    "            answer_dic[question_num] = []\n",
    "\n",
    "        else:\n",
    "            sentence = answer_sentences[i].split(' ')\n",
    "            pro = 0\n",
    "            \n",
    "            denominator_list = []\n",
    "            \n",
    "            # 前n-1个词，分母初始化\n",
    "            \n",
    "            for k in range(n - 1):\n",
    "                denominator_list.append('#')\n",
    "\n",
    "\n",
    "            for k in range(0,len(sentence)):\n",
    "                \n",
    "                word_now = sentence[k]\n",
    "                \n",
    "                # 计算分母\n",
    "                denominator_string = ''\n",
    "                for word in denominator_list:\n",
    "                    denominator_string += word\n",
    "                    \n",
    "                # 分子字符串\n",
    "                numerator_string = denominator_string+word_now\n",
    "                \n",
    "                #print(numerator.get(numerator_string,0),(denominator.get(denominator_string,0)))\n",
    "                \n",
    "                pro += math.log((numerator.get(numerator_string,0)+smooth)/(denominator.get(denominator_string,0)+smooth*voca_size))\n",
    "                \n",
    "                # 更新\n",
    "                for j in range(len(denominator_list) - 1):\n",
    "                    denominator_list[j] = denominator_list[j+1]\n",
    "                denominator_list[-1] = word_now\n",
    "                \n",
    "            # 计算分母\n",
    "            denominator_string = ''\n",
    "            for word in denominator_list:\n",
    "                denominator_string +=word\n",
    "            \n",
    "            # 计算分子\n",
    "            word_now = '&'\n",
    "            numerator_string = denominator_string +  word_now\n",
    "            pro += \\\n",
    "                math.log((numerator.get(numerator_string,0)+smooth)/(denominator.get(denominator_string,0)+smooth))\n",
    "\n",
    "            answer_dic[question_num].append(pro)\n",
    "            \n",
    "    answer_list = []\n",
    "    compare = ['a','b','c','d','e']\n",
    "\n",
    "    for k,v in answer_dic.items():\n",
    "        maxIndex = v.index(max(v))\n",
    "        answer_list.append(str(compare[maxIndex]))\n",
    "            \n",
    "    return answer_dic,answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smooth = 1\n",
    "print('smooth: ',smooth)\n",
    "\n",
    "for n in range(2,6):\n",
    "    numerator,denominator = n_gram(n,train_words)\n",
    "    answer_dic,answer_list = get_pre(n,numerator,denominator,answer_sentences,voca_size,smooth)\n",
    "    print(str(n)+'-gram:')\n",
    "    print('概率:',end='')\n",
    "    eval_pre(answer_list)\n",
    "    answer_dic,answer_list = get_pre(n,numerator,{},answer_sentences,1,smooth)\n",
    "    print('频率:',end='')\n",
    "    eval_pre(answer_list)\n",
    "\n",
    "smooth = 0.5\n",
    "print('smooth: ',smooth)\n",
    "print()\n",
    "\n",
    "for n in range(2,6):\n",
    "    numerator,denominator = n_gram(n,train_words)\n",
    "    answer_dic,answer_list = get_pre(n,numerator,denominator,answer_sentences,voca_size,smooth)\n",
    "    print(str(n)+'-gram:')\n",
    "    print('概率:',end='')\n",
    "    eval_pre(answer_list)\n",
    "    answer_dic,answer_list = get_pre(n,numerator,{},answer_sentences,1,smooth)\n",
    "    print('频率:',end='')\n",
    "    eval_pre(answer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gensim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim word2vec score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练词向量 参数 (hs=1,negative=0,sample=0.001,window=2,size=200,min_count=10,workers =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## gensim\n",
    "\n",
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 引入日志配置\n",
    "#import logging\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 引入数据集\n",
    "raw_sentences = open('./train-sentences.txt').read()\n",
    "\n",
    "\n",
    "\n",
    "raw_sentences = raw_sentences.split('\\n')\n",
    "\n",
    "# 切分词汇\n",
    "sentences = [s.split(' ') for s in raw_sentences]\n",
    "\n",
    "# 构建word2vec\n",
    "model = word2vec.Word2Vec(sentences,hs=1,negative=0,sample=0.001,window=10,size=300,min_count=10,workers = 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 读取dev set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# 引入数据集\n",
    "raw_sentences = open('./dev-sentences.txt').read()\n",
    "answer_sentences = raw_sentences.split('\\n')\n",
    "answer_sentences = answer_sentences[:-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pre_wv(answer_sentences,model):\n",
    "    answer_dic = {}\n",
    "\n",
    "    for i in range(len(answer_sentences)):\n",
    "        \n",
    "        if i%6 == 0:\n",
    "            question_num = answer_sentences[i]\n",
    "            answer_dic[question_num] = []\n",
    "\n",
    "        else:\n",
    "            sentence = answer_sentences[i].split(' ')\n",
    "            answer_dic[question_num].append(model.score([answer_sentences[i].split(' ')]))\n",
    "            \n",
    "            \n",
    "    answer_list = []\n",
    "    compare = ['a','b','c','d','e']\n",
    "    \n",
    "\n",
    "    for k,v in answer_dic.items():\n",
    "        maxIndex = v.index(max(v))\n",
    "        answer_list.append(str(compare[maxIndex]))\n",
    "            \n",
    "    return answer_dic,answer_list\n",
    "\n",
    "answer_dic,answer_list = get_pre_wv(answer_sentences,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### dev正确率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pre(answer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## gensim word2vec predict_output_word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练词向量 参数 (hs=1,window=7,size=300,min_count=1,workers =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 引入日志配置\n",
    "#import logging\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 引入数据集\n",
    "raw_sentences = open('./train-sentences.txt').read()\n",
    "\n",
    "\n",
    "\n",
    "raw_sentences = raw_sentences.split('\\n')\n",
    "\n",
    "# 切分词汇\n",
    "sentences = [s.split(' ') for s in raw_sentences]\n",
    "\n",
    "# 构建模型\n",
    "model = word2vec.Word2Vec(sentences,hs=1,window=10,size=300,min_count=5,workers =7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备上下文和答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dev = open('./development_set.txt').read()\n",
    "\n",
    "dev = re.sub(r\"\\n\\n\\n\",'#', dev)\n",
    "\n",
    "dev_questions = dev.split('#')\n",
    "dev_questions = dev_questions[0:-1]\n",
    "\n",
    "\n",
    "answer_abcde_dic = {}\n",
    "question_dic = {}\n",
    "\n",
    "for dev_question in dev_questions:\n",
    "    \n",
    "    question_num = dev_question[:dev_question.find(')')]\n",
    "    question = dev_question[dev_question.find(' ')+1:dev_question.find('\\n')]\n",
    "    \n",
    "    \n",
    "    \n",
    "    a_answer = dev_question[dev_question.find('a)')+3:dev_question.find('\\n',dev_question.find('a)')+3)]\n",
    "    b_answer = dev_question[dev_question.find('b)')+3:dev_question.find('\\n',dev_question.find('b)')+3)]\n",
    "    c_answer = dev_question[dev_question.find('c)')+3:dev_question.find('\\n',dev_question.find('c)')+3)]\n",
    "    d_answer = dev_question[dev_question.find('d)')+3:dev_question.find('\\n',dev_question.find('d)')+3)]\n",
    "    e_answer = dev_question[dev_question.find('e)')+3:]\n",
    "    \n",
    "    question_dic[question_num] = question.replace('_____','')\n",
    "    \n",
    "    answer_abcde_dic[question_num] = []\n",
    "    answer_abcde_dic[question_num].append(a_answer)\n",
    "    answer_abcde_dic[question_num].append(b_answer)\n",
    "    answer_abcde_dic[question_num].append(c_answer)\n",
    "    answer_abcde_dic[question_num].append(d_answer)\n",
    "    answer_abcde_dic[question_num].append(e_answer)\n",
    "    \n",
    "import string\n",
    "for key in question_dic.keys():\n",
    "    for letter in string.punctuation:\n",
    "        question_dic[key] = question_dic[key].replace(letter,' ')\n",
    "        \n",
    "# 多个' '变一个' '，去掉前后的' '\n",
    "for key in question_dic.keys():\n",
    "    question_dic[key] = re.sub(' +',' ',question_dic[key])\n",
    "    question_dic[key] = question_dic[key].strip() \n",
    "# 变成小写\n",
    "for key in question_dic.keys():\n",
    "    question_dic[key] = question_dic[key].lower()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 得到预测的概率"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_pre_wv_pro(question_dic,answer_abcde_dic,model):\n",
    "    answer_dic = {}\n",
    "\n",
    "    for key in question_dic.keys():\n",
    "        \n",
    "        answer_dic[key] = []\n",
    "        \n",
    "        tupeList = model.predict_output_word(question_dic[key].split(' '),topn=40000)\n",
    "        dicList = {}\n",
    "        for tup in tupeList:\n",
    "            dicList[tup[0]] = tup[1]\n",
    "        \n",
    "        \n",
    "        for answer in answer_abcde_dic[key]:\n",
    "            pro = 0\n",
    "            if answer in dicList:\n",
    "                pro = dicList[answer]\n",
    "                \n",
    "            answer_dic[key].append(pro)\n",
    "            \n",
    "        \n",
    "            \n",
    "    answer_list = []\n",
    "    compare = ['a','b','c','d','e']\n",
    "\n",
    "    for k,v in answer_dic.items():\n",
    "        maxIndex = v.index(max(v))\n",
    "        answer_list.append(str(compare[maxIndex]))\n",
    "            \n",
    "    return answer_dic,answer_list\n",
    "\n",
    "\n",
    "answer_dic,answer_list = get_pre_wv_pro(question_dic,answer_abcde_dic,model)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pre(answer_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 相似度比较"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 训练词向量 参数 (hs=1,window=7,size=300,min_count=1,workers =4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "from gensim.models import KeyedVectors\n",
    "\n",
    "# 引入日志配置\n",
    "#import logging\n",
    "\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "# 引入数据集\n",
    "raw_sentences = open('./train-sentences.txt').read()\n",
    "\n",
    "\n",
    "\n",
    "raw_sentences = raw_sentences.split('\\n')\n",
    "\n",
    "# 切分词汇\n",
    "sentences = [s.split(' ') for s in raw_sentences]\n",
    "\n",
    "# 构建模型\n",
    "model = word2vec.Word2Vec(sentences,hs=1,window=10,size=300,min_count=5,workers =7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 准备上下文和答案"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "dev = open('./development_set.txt').read()\n",
    "\n",
    "dev = re.sub(r\"\\n\\n\\n\",'#', dev)\n",
    "\n",
    "dev_questions = dev.split('#')\n",
    "print(len(dev_questions))\n",
    "dev_questions = dev_questions[0:-1]\n",
    "print(len(dev_questions))\n",
    "\n",
    "\n",
    "answer_abcde_dic = {}\n",
    "question_dic = {}\n",
    "\n",
    "for dev_question in dev_questions:\n",
    "    \n",
    "    question_num = dev_question[:dev_question.find(')')]\n",
    "    question = dev_question[dev_question.find(' ')+1:dev_question.find('\\n')]\n",
    "    \n",
    "    \n",
    "    \n",
    "    a_answer = dev_question[dev_question.find('a)')+3:dev_question.find('\\n',dev_question.find('a)')+3)]\n",
    "    b_answer = dev_question[dev_question.find('b)')+3:dev_question.find('\\n',dev_question.find('b)')+3)]\n",
    "    c_answer = dev_question[dev_question.find('c)')+3:dev_question.find('\\n',dev_question.find('c)')+3)]\n",
    "    d_answer = dev_question[dev_question.find('d)')+3:dev_question.find('\\n',dev_question.find('d)')+3)]\n",
    "    e_answer = dev_question[dev_question.find('e)')+3:]\n",
    "    \n",
    "    question_dic[question_num] = question.replace('_____','')\n",
    "    \n",
    "    answer_abcde_dic[question_num] = []\n",
    "    answer_abcde_dic[question_num].append(a_answer)\n",
    "    answer_abcde_dic[question_num].append(b_answer)\n",
    "    answer_abcde_dic[question_num].append(c_answer)\n",
    "    answer_abcde_dic[question_num].append(d_answer)\n",
    "    answer_abcde_dic[question_num].append(e_answer)\n",
    "\n",
    "import string\n",
    "for key in question_dic.keys():\n",
    "    for letter in string.punctuation:\n",
    "        question_dic[key] = question_dic[key].replace(letter,' ')\n",
    "        \n",
    "# 多个' '变一个' '，去掉前后的' '\n",
    "for key in question_dic.keys():\n",
    "    question_dic[key] = re.sub(' +',' ',question_dic[key])\n",
    "    question_dic[key] = question_dic[key].strip() \n",
    "# 变成小写\n",
    "for key in question_dic.keys():\n",
    "    question_dic[key] = question_dic[key].lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_total_similarity(question_dic,answer_dic,model):\n",
    "    answer_dic = {}\n",
    "\n",
    "    for key,v in question_dic.items():\n",
    "\n",
    "        answer_dic[key] = []\n",
    "\n",
    "        for answer in answer_abcde_dic[key]:\n",
    "\n",
    "            similarity = 0\n",
    "            words = v.split(' ')\n",
    "            for word in words:\n",
    "                if word in model and answer in model:\n",
    "                    similarity+=model.similarity(answer,word)\n",
    "            answer_dic[key].append(similarity)\n",
    "    answer_list = []\n",
    "    compare = ['a','b','c','d','e']\n",
    "\n",
    "    for k,v in answer_dic.items():\n",
    "        maxIndex = v.index(max(v))\n",
    "        answer_list.append(str(compare[maxIndex]))\n",
    "\n",
    "    return answer_dic,answer_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_dic,answer_list = get_total_similarity(question_dic,answer_abcde_dic,model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 评价"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_pre(answer_list)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
